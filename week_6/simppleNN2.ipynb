# implement a simple neural network with 1 hidden layer with 4 neurons

import numpy as np
import matplotlib.pyplot as plt

# define the sigmoid function
def sigmoid(x):
    return 1/(1+np.exp(-x))

# define the derivative of the sigmoid function
def sigmoid_derivative(x):
    return x*(1-x)

# input dataset
data = np.random.uniform(0,1,(100,2))
fitdata = np.sign(data[:, 0]*data[:, 1])

# initialize weights randomly with mean 0
w0 = 2*np.random.random((2,4)) - 1
w1 = 2*np.random.random((4,1)) - 1

# initialize the bias
b0 = np.zeros((1,4))
b1 = np.zeros((1,1))

# initialize the learning rate
eta = 0.1

# initialize the number of epochs
epochs = 10000

# initialize the error
error = np.zeros(epochs)

# train the network
for i in range(epochs):
    # forward propagation
    l0 = data
    l1 = sigmoid(np.dot(l0,w0)+b0)
    l2 = sigmoid(np.dot(l1,w1)+b1)
    # compute the error
    error[i] = np.sum(np.abs(fitdata - l2))
    # back propagation
    l2_error = fitdata - l2
    l2_delta = l2_error*sigmoid_derivative(l2)
    l1_error = l2_delta.dot(w1.T)
    l1_delta = l1_error*sigmoid_derivative(l1)
    # update the weights
    w1 += eta*l1.T.dot(l2_delta)
    w0 += eta*l0.T.dot(l1_delta)

# plot the error
plt.plot(error)
plt.xlabel('Epochs')
plt.ylabel('Error')
plt.show()



